{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the parent directory of the notebook (project root)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"CUDA_VISIBLE_DEVICES\") is None:\n",
    "    gpu_num = 0  # Use \"\" to use the CPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna\n",
    "\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "sionna.config.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sionna.utils import ebnodb2no\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.channels.rt_channel import (\n",
    "    sampling_rt_channel_freq,\n",
    "    sampling_rt_channel_time,\n",
    "    no,\n",
    ")\n",
    "from src.channels.channel_est.ls_channel import ChannelEstimator\n",
    "from src.channels.channel_est.ml_channel import VAE\n",
    "from src.channels.lmmse_equalizer import lmmse_equalizer\n",
    "from src.data.binary_sources import binary_sources\n",
    "from src.data.qam_demapper import qam_demapper\n",
    "from src.data.qam_mapper import qam_mapper\n",
    "from src.data.response import response_freqency_domain, response_time_domain\n",
    "from src.evals.ber import ber\n",
    "from src.ldpc.ldpc_decoder import ldpc_decoder\n",
    "from src.ldpc.ldpc_encoder import ldpc_encoder\n",
    "from src.ml.gen_data import get_pilot_matrix, remove_nulls_subcarriers\n",
    "from src.ml.transform import MinMaxScaler4D\n",
    "from src.ml.utils import (\n",
    "    compute_mean_std,\n",
    "    nmse_func,\n",
    "    reshape_data,\n",
    "    reversed_reshape_data,\n",
    "    ssim_func,\n",
    ")\n",
    "from src.ofdm.ofdm_demodulation import ofdm_demodulation\n",
    "from src.ofdm.ofdm_modulation import ofdm_modulation\n",
    "from src.ofdm.ofdm_resource_grids import resource_grid_mapper, rg\n",
    "from src.settings.config import (\n",
    "    batch_size,\n",
    "    bits_per_symbol,\n",
    "    code_rate,\n",
    "    num_bs_ant,\n",
    "    num_streams_per_tx,\n",
    "    num_tx,\n",
    "    num_ut_ant,\n",
    "    number_of_bits,\n",
    "    speed,\n",
    ")\n",
    "from src.settings.ml import device, number_of_samples\n",
    "\n",
    "scaler = MinMaxScaler4D()\n",
    "h_freq_shape = remove_nulls_subcarriers(sampling_rt_channel_freq()).shape\n",
    "root_dir = \"/home/thinh/TND-Lab/CDL_Channel_Estimation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_model():\n",
    "    src_dir = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}_speed_{speed}_samples_{number_of_samples}_ebno_0\"\n",
    "    h_freqs = np.load(f\"{root_dir}/data/{src_dir}/h_freqs.npy\")\n",
    "\n",
    "    h_freqs = reshape_data(h_freqs)\n",
    "\n",
    "    h_freqs = scaler.fit_transform(h_freqs)\n",
    "\n",
    "    mu_h, std_h = compute_mean_std(h_freqs)\n",
    "    mu_h = mu_h.to(device)\n",
    "    std_h = std_h.to(device)\n",
    "\n",
    "    vae = VAE(\n",
    "        h_freqs.shape[1:],\n",
    "        mu_h,\n",
    "        std_h,\n",
    "    )\n",
    "\n",
    "    vae.load_state_dict(\n",
    "        torch.load(\n",
    "            f\"{root_dir}/results/checkpoints/{src_dir}/vae_fold_2_best.pth\",\n",
    "            map_location=device,\n",
    "        )[\"state_dict\"]\n",
    "    )\n",
    "    vae = vae.to(device)\n",
    "    vae.eval()\n",
    "\n",
    "    return vae\n",
    "\n",
    "\n",
    "vae = set_up_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmse_channel_estimation_ai(number_of_trials, no=no):\n",
    "    nmse_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_rt_channel_freq()\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        pilot_matrix = get_pilot_matrix(response_symbols)\n",
    "        # pilot_matrix = replicate_to_shape(pilot_matrix, h_freq_shape)\n",
    "        pilot_matrix = reshape_data(pilot_matrix)\n",
    "        pilot_matrix = scaler.fit_transform(pilot_matrix, False).to(device)\n",
    "        h_est_hat, *values = vae(pilot_matrix)\n",
    "        h_est_hat = scaler.inverse_transform(h_est_hat.cpu().detach())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        nmse_score = nmse_func(\n",
    "            torch.from_numpy(h_est),\n",
    "            torch.from_numpy(h_est_hat),\n",
    "        )\n",
    "        nmse_scores.append(nmse_score)\n",
    "\n",
    "    return np.mean(nmse_scores)\n",
    "\n",
    "\n",
    "def nmse_channel_estimation_ls(number_of_trials, inter: str = \"nn\", no=no, order=\"t-f\"):\n",
    "    nmse_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_rt_channel_freq()\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        channel_estimator = ChannelEstimator(interpolation_factor=inter, order=order)\n",
    "        h_est_hat, err_var = channel_estimator.estimate(response_symbols)\n",
    "\n",
    "        h_est_hat = reshape_data(h_est_hat.numpy())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        nmse_score = nmse_func(\n",
    "            torch.from_numpy(h_est_hat),\n",
    "            torch.from_numpy(h_est),\n",
    "        )\n",
    "        nmse_scores.append(nmse_score)\n",
    "\n",
    "    return np.mean(nmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ai(1)\n",
    "end = time.time()\n",
    "print(f\"Time taken for AI: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ls(1, inter=\"nn\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+NN: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ls(1, inter=\"lin\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+Lin: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ls(1, inter=\"nn\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+NN: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "hyperparameters = {\n",
    "    \"ebno_dbs\": np.arange(-20, 21, 5),\n",
    "    \"speeds\": [0],\n",
    "}\n",
    "\n",
    "number_of_trials = 200\n",
    "nmse_scores = {}\n",
    "inference_times = {}  # Dictionary to store inference times\n",
    "\n",
    "for speed in hyperparameters[\"speeds\"]:\n",
    "    for ebno_db in hyperparameters[\"ebno_dbs\"]:\n",
    "        nmse_score = {}\n",
    "        inference_time = {}  # Store times for each model\n",
    "        no = ebnodb2no(ebno_db, bits_per_symbol, code_rate, rg)\n",
    "\n",
    "        nmse_score[\"vae\"] = nmse_channel_estimation_ai(number_of_trials, no=no)\n",
    "        nmse_score[\"ls+nn\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"nn\", no=no\n",
    "        )\n",
    "        nmse_score[\"ls+lin\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lin\", no=no\n",
    "        )\n",
    "        nmse_score[\"ls+lmmse: t-f\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", no=no, order=\"t-f\"\n",
    "        )\n",
    "        nmse_score[\"ls+lmmse: t-f-s\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", no=no, order=\"t-f-s\"\n",
    "        )\n",
    "        # nmse_score[\"ls+lmmse: s-t-f\"] = nmse_channel_estimation_ls(number_of_trials, inter=\"lmmse\", no=no, order='s-t-f')\n",
    "        nmse_scores[f\"speed_{speed}_ebno_db_{ebno_db}\"] = nmse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval_rt/nmse/nmse.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = nmse_scores\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_channel_estimation_ai(number_of_trials, no=no):\n",
    "    ssim_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_rt_channel_freq()\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        pilot_matrix = get_pilot_matrix(response_symbols)\n",
    "        # pilot_matrix = replicate_to_shape(pilot_matrix, h_freq_shape)\n",
    "        pilot_matrix = reshape_data(pilot_matrix)\n",
    "        pilot_matrix = scaler.fit_transform(pilot_matrix, False).to(device)\n",
    "        h_est_hat, *values = vae(pilot_matrix)\n",
    "        h_est_hat = scaler.inverse_transform(h_est_hat.cpu().detach())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        print(h_est_hat.shape)\n",
    "        print(h_est.shape)\n",
    "\n",
    "        ssim_score = ssim_func(\n",
    "            torch.from_numpy(h_est),\n",
    "            torch.from_numpy(h_est_hat),\n",
    "        )\n",
    "        ssim_scores.append(ssim_score)\n",
    "\n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "\n",
    "def ssim_channel_estimation_ls(number_of_trials, inter: str = \"nn\", no=no, order=\"t-f\"):\n",
    "    ssim_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_rt_channel_freq()\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        channel_estimator = ChannelEstimator(interpolation_factor=inter, order=order)\n",
    "        h_est_hat, err_var = channel_estimator.estimate(response_symbols)\n",
    "\n",
    "        h_est_hat = reshape_data(h_est_hat.numpy())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        ssim_score = ssim_func(\n",
    "            torch.from_numpy(h_est_hat),\n",
    "            torch.from_numpy(h_est),\n",
    "        )\n",
    "        ssim_scores.append(ssim_score)\n",
    "\n",
    "    return np.mean(ssim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ai(1)\n",
    "end = time.time()\n",
    "print(f\"Time taken for AI: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ls(1, inter=\"nn\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+NN: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ls(1, inter=\"lin\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+Lin: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ls(1, inter=\"lmmse\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+LMMSE: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "hyperparameters = {\n",
    "    \"ebno_dbs\": np.arange(-20, 21, 5),\n",
    "    \"speeds\": [0],\n",
    "}\n",
    "\n",
    "number_of_trials = 200\n",
    "ssim_scores = {}\n",
    "inference_times = {}  # Dictionary to store inference times\n",
    "\n",
    "for speed in hyperparameters[\"speeds\"]:\n",
    "    for ebno_db in hyperparameters[\"ebno_dbs\"]:\n",
    "        ssim_score = {}\n",
    "        inference_time = {}  # Store times for each model\n",
    "        no = ebnodb2no(ebno_db, bits_per_symbol, code_rate, rg)\n",
    "\n",
    "        ssim_score[\"vae\"] = ssim_channel_estimation_ai(number_of_trials, no=no)\n",
    "        ssim_score[\"ls+nn\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"nn\", no=no\n",
    "        )\n",
    "        ssim_score[\"ls+lin\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lin\", no=no\n",
    "        )\n",
    "        ssim_score[\"ls+lmmse: t-f\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", no=no, order=\"t-f\"\n",
    "        )\n",
    "        ssim_score[\"ls+lmmse: t-f-s\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", no=no, order=\"t-f-s\"\n",
    "        )\n",
    "        # ssim_score[\"ls+lmmse: s-t-f\"] = ssim_channel_estimation_ls(number_of_trials, inter=\"lmmse\", no=no, order='s-t-f')\n",
    "        ssim_scores[f\"speed_{speed}_ebno_db_{ebno_db}\"] = ssim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval_rt/ssim/ssim.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = ssim_scores\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ber_channel_estimation_ai(number_of_trials, no=no):\n",
    "    ber_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        modulated_qam_symbols = ofdm_modulation(mapped_qam_symbol)\n",
    "        h_time = sampling_rt_channel_time()\n",
    "        response_symbols = response_time_domain(modulated_qam_symbols, h_time, no=no)\n",
    "        demodulated_response_symbols = ofdm_demodulation(response_symbols)\n",
    "\n",
    "        # Channel estimation\n",
    "        pilot_matrix = get_pilot_matrix(demodulated_response_symbols)\n",
    "        # pilot_matrix = replicate_to_shape(pilot_matrix, h_freq_shape)\n",
    "        pilot_matrix = reshape_data(pilot_matrix)\n",
    "        pilot_matrix = scaler.fit_transform(pilot_matrix, False).to(device)\n",
    "        h_est_hat, *values = vae(pilot_matrix)\n",
    "        h_est_hat = scaler.inverse_transform(h_est_hat.cpu().detach())\n",
    "        h_est_hat = reversed_reshape_data(h_est_hat)\n",
    "\n",
    "        mapped_qam_symbol_hat, no_eff = lmmse_equalizer(\n",
    "            demodulated_response_symbols, h_est_hat, 0\n",
    "        )\n",
    "\n",
    "        binary_values_hat = qam_demapper(mapped_qam_symbol_hat, bits_per_symbol, no_eff)\n",
    "\n",
    "        decoded_binary_values_hat = ldpc_decoder(binary_values_hat)\n",
    "\n",
    "        error_rate = ber(binary_values, decoded_binary_values_hat)\n",
    "        ber_scores.append(error_rate)\n",
    "\n",
    "    return np.mean(ber_scores)\n",
    "\n",
    "\n",
    "def ber_channel_estimation_ls(number_of_trials, inter: str = \"nn\", no=no, order=\"t-f\"):\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        modulated_qam_symbols = ofdm_modulation(mapped_qam_symbol)\n",
    "        h_time = sampling_rt_channel_time()\n",
    "        response_symbols = response_time_domain(modulated_qam_symbols, h_time, no=no)\n",
    "        demodulated_response_symbols = ofdm_demodulation(response_symbols)\n",
    "\n",
    "        channel_estimator = ChannelEstimator(interpolation_factor=inter, order=order)\n",
    "        h_est_hat, err_var = channel_estimator.estimate(demodulated_response_symbols)\n",
    "        mapped_qam_symbol_hat, no_eff = lmmse_equalizer(\n",
    "            demodulated_response_symbols, h_est_hat, err_var\n",
    "        )\n",
    "\n",
    "        binary_values_hat = qam_demapper(mapped_qam_symbol_hat, bits_per_symbol, no_eff)\n",
    "\n",
    "        decoded_binary_values_hat = ldpc_decoder(binary_values_hat)\n",
    "\n",
    "        error_rate = ber(binary_values, decoded_binary_values_hat)\n",
    "\n",
    "    return float(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ber_score = ber_channel_estimation_ai(1)\n",
    "print(\"BER AI: \", ber_score)\n",
    "end = time.time()\n",
    "print(f\"Time taken for AI: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ber_score = ber_channel_estimation_ai(1)\n",
    "print(\"BER AI: \", ber_score)\n",
    "end = time.time()\n",
    "print(f\"Time taken for AI: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ber_score = ber_channel_estimation_ls(1, inter=\"lin\")\n",
    "print(\"BER LS+Lin: \", ber_score)\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+Lin: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ber_score = ber_channel_estimation_ls(1, inter=\"lmmse\")\n",
    "print(\"BER LS+LMMSE: \", ber_score)\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+LMMSE: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "hyperparameters = {\n",
    "    \"ebno_dbs\": np.arange(-20, 21, 5),\n",
    "    \"speeds\": [0],\n",
    "}\n",
    "\n",
    "number_of_trials = 200\n",
    "ber_scores = {}\n",
    "inference_times = {}  # Dictionary to store inference times\n",
    "\n",
    "for speed in hyperparameters[\"speeds\"]:\n",
    "    \n",
    "    for ebno_db in hyperparameters[\"ebno_dbs\"]:\n",
    "        ber_score = {}\n",
    "        inference_time = {}  # Store times for each model\n",
    "        no = ebnodb2no(ebno_db, bits_per_symbol, code_rate, rg)\n",
    "\n",
    "        # Measure inference time for VAE\n",
    "        start_time = time.time()\n",
    "        ber_score[\"vae: no-scaled\"] = ber_channel_estimation_ai(\n",
    "            number_of_trials, no=no,\n",
    "        )\n",
    "        inference_time[\"vae: no-scaled\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        ber_score[\"ls+nn\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"nn\", no=no\n",
    "        )\n",
    "        ber_score[\"ls+lin\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lin\", no=no\n",
    "        )\n",
    "        ber_score[\"ls+lmmse: t-f\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", no=no, order=\"t-f\"\n",
    "        )\n",
    "        ber_score[\"ls+lmmse: t-f-s\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", no=no, order=\"t-f-s\"\n",
    "        )\n",
    "        # ber_score[\"ls+lmmse: s-t-f\"] = ber_channel_estimation_ls(number_of_trials, inter=\"lmmse\", no=no, order='s-t-f')\n",
    "        ber_scores[f\"speed_{speed}_ebno_db_{ebno_db}\"] = ber_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval_rt/ber/ber.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = ber_scores\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ele6709",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
