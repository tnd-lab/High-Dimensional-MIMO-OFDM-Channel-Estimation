{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the parent directory of the notebook (project root)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"CUDA_VISIBLE_DEVICES\") is None:\n",
    "    gpu_num = 0  # Use \"\" to use the CPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna\n",
    "\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "sionna.config.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sionna.channel.tr38901 import CDL\n",
    "from sionna.utils import ebnodb2no\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.channels.cdl_channel import (\n",
    "    cdl,\n",
    "    sampling_channel_freq,\n",
    "    sampling_channel_time,\n",
    "    no,\n",
    ")\n",
    "from src.channels.channel_est.ls_channel import ChannelEstimator\n",
    "from src.channels.channel_est.ml_channel import VAE\n",
    "from src.channels.lmmse_equalizer import lmmse_equalizer\n",
    "from src.data.binary_sources import binary_sources\n",
    "from src.data.qam_demapper import qam_demapper\n",
    "from src.data.qam_mapper import qam_mapper\n",
    "from src.data.response import response_freqency_domain, response_time_domain\n",
    "from src.evals.ber import ber\n",
    "from src.ldpc.ldpc_decoder import ldpc_decoder\n",
    "from src.ldpc.ldpc_encoder import ldpc_encoder\n",
    "from src.ml.gen_data import get_pilot_matrix, remove_nulls_subcarriers\n",
    "from src.ml.transform import MinMaxScaler4D\n",
    "from src.ml.utils import (\n",
    "    compute_mean_std,\n",
    "    nmse_func,\n",
    "    reshape_data,\n",
    "    reversed_reshape_data,\n",
    "    replicate_to_shape,\n",
    "    ssim_func,\n",
    ")\n",
    "from src.ofdm.ofdm_demodulation import ofdm_demodulation\n",
    "from src.ofdm.ofdm_modulation import ofdm_modulation\n",
    "from src.ofdm.ofdm_resource_grids import resource_grid_mapper, rg\n",
    "from src.settings.antenna import bs_array, ut_array\n",
    "from src.settings.config import (\n",
    "    batch_size,\n",
    "    bits_per_symbol,\n",
    "    carrier_frequency,\n",
    "    cdl_model,\n",
    "    code_rate,\n",
    "    delay_spread,\n",
    "    direction,\n",
    "    ebno_db,\n",
    "    num_bs_ant,\n",
    "    num_effective_subcarriers,\n",
    "    num_ofdm_symbols,\n",
    "    num_rx,\n",
    "    num_streams_per_tx,\n",
    "    num_tx,\n",
    "    num_ut_ant,\n",
    "    number_of_bits,\n",
    "    speed,\n",
    ")\n",
    "from src.settings.ml import device, number_of_samples\n",
    "from src.utils.plots import plot_channel_frequency_domain, plot_symbols\n",
    "\n",
    "scaler = MinMaxScaler4D()\n",
    "h_freq_shape = remove_nulls_subcarriers(sampling_channel_freq()).shape\n",
    "root_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_model():\n",
    "    src_dir = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}_speed_{speed}_samples_{number_of_samples}_ebno_0\"\n",
    "    h_freqs = np.load(f\"{root_dir}/data/{src_dir}/h_freqs.npy\")\n",
    "\n",
    "    h_freqs = reshape_data(h_freqs)\n",
    "\n",
    "    h_freqs = scaler.fit_transform(h_freqs)\n",
    "\n",
    "    mu_h, std_h = compute_mean_std(h_freqs)\n",
    "    mu_h = mu_h.to(device)\n",
    "    std_h = std_h.to(device)\n",
    "\n",
    "    vae = VAE(\n",
    "        h_freqs.shape[1:],\n",
    "        mu_h,\n",
    "        std_h,\n",
    "    )\n",
    "\n",
    "    vae.load_state_dict(\n",
    "        torch.load(\n",
    "            f\"{root_dir}/results/checkpoints/{src_dir}/vae_fold_2_best.pth\",\n",
    "            map_location=device,\n",
    "        )[\"state_dict\"]\n",
    "    )\n",
    "    vae = vae.to(device)\n",
    "    vae.eval()\n",
    "\n",
    "    return vae\n",
    "\n",
    "\n",
    "vae = set_up_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmse_channel_estimation_ai(number_of_trials, no=no, cdl=cdl):\n",
    "    nmse_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_channel_freq(cdl=cdl)\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        pilot_matrix = get_pilot_matrix(response_symbols)\n",
    "        # pilot_matrix = replicate_to_shape(pilot_matrix, h_freq_shape)\n",
    "        pilot_matrix = reshape_data(pilot_matrix)\n",
    "        pilot_matrix = scaler.fit_transform(pilot_matrix, False).to(device)\n",
    "        h_est_hat, *values = vae(pilot_matrix)\n",
    "        h_est_hat = scaler.inverse_transform(h_est_hat.cpu().detach())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        nmse_score = nmse_func(\n",
    "            torch.from_numpy(h_est),\n",
    "            torch.from_numpy(h_est_hat),\n",
    "        )\n",
    "        nmse_scores.append(nmse_score)\n",
    "\n",
    "    return np.mean(nmse_scores)\n",
    "\n",
    "\n",
    "def nmse_channel_estimation_ls(\n",
    "    number_of_trials, inter: str = \"nn\", cdl=cdl, no=no, order=\"t-f\"\n",
    "):\n",
    "    nmse_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_channel_freq(cdl=cdl)\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        channel_estimator = ChannelEstimator(interpolation_factor=inter, order=order)\n",
    "        h_est_hat, err_var = channel_estimator.estimate(response_symbols)\n",
    "\n",
    "        h_est_hat = reshape_data(h_est_hat.numpy())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        nmse_score = nmse_func(\n",
    "            torch.from_numpy(h_est_hat),\n",
    "            torch.from_numpy(h_est),\n",
    "        )\n",
    "        nmse_scores.append(nmse_score)\n",
    "\n",
    "    return np.mean(nmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for AI: 0.2380504608154297 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ai(1)\n",
    "end = time.time()\n",
    "print(f\"Time taken for AI: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LS+NN: 0.06303930282592773 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ls(1, inter=\"nn\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+NN: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LS+Lin: 0.060755014419555664 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ls(1, inter=\"lin\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+Lin: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LS+LMMSE: 0.42087650299072266 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nmse_score = nmse_channel_estimation_ls(1, inter=\"lmmse\", order=\"s-t-f\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+LMMSE: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 20.79it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.08it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.08it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.15it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.10it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.33it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.02it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.16it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.88it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.23it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.08it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.26it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.11it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.17it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.25it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.00it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.06it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.36it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.23it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.06it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.08it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.14it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.99it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.92it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.02it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.09it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "hyperparameters = {\n",
    "    \"ebno_dbs\": np.arange(-20, 21, 5),\n",
    "    \"speeds\": np.arange(0, 11, 5),\n",
    "}\n",
    "\n",
    "number_of_trials = 200\n",
    "nmse_scores = {}\n",
    "inference_times = {}  # Dictionary to store inference times\n",
    "\n",
    "for speed in hyperparameters[\"speeds\"]:\n",
    "    cdl = CDL(\n",
    "        cdl_model,\n",
    "        delay_spread,\n",
    "        carrier_frequency,\n",
    "        ut_array,\n",
    "        bs_array,\n",
    "        direction,\n",
    "        min_speed=speed,\n",
    "    )\n",
    "    for ebno_db in hyperparameters[\"ebno_dbs\"]:\n",
    "        nmse_score = {}\n",
    "        inference_time = {}  # Store times for each model\n",
    "        no = ebnodb2no(ebno_db, bits_per_symbol, code_rate, rg)\n",
    "\n",
    "        # Measure inference time for VAE\n",
    "        start_time = time.time()\n",
    "        nmse_score[\"vae\"] = nmse_channel_estimation_ai(number_of_trials, no=no, cdl=cdl)\n",
    "        inference_time[\"vae\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+NN\n",
    "        start_time = time.time()\n",
    "        nmse_score[\"ls+nn\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"nn\", cdl=cdl, no=no\n",
    "        )\n",
    "        inference_time[\"ls+nn\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LIN\n",
    "        start_time = time.time()\n",
    "        nmse_score[\"ls+lin\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lin\", cdl=cdl, no=no\n",
    "        )\n",
    "        inference_time[\"ls+lin\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        nmse_score[\"ls+lmmse: t-f\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"t-f\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: t-f\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        nmse_score[\"ls+lmmse: t-f-s\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"t-f-s\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: t-f-s\"] = (\n",
    "            time.time() - start_time\n",
    "        ) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        nmse_score[\"ls+lmmse: s-t-f\"] = nmse_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"s-t-f\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: s-t-f\"] = (\n",
    "            time.time() - start_time\n",
    "        ) / number_of_trials\n",
    "\n",
    "        nmse_scores[f\"speed_{speed}_ebno_db_{ebno_db}\"] = nmse_score\n",
    "        inference_times[f\"speed_{speed}_ebno_db_{ebno_db}\"] = inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval/nmse/nmse.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = nmse_scores\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval/nmse/time.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = inference_times\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_channel_estimation_ai(number_of_trials, no=no, cdl=cdl):\n",
    "    ssim_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_channel_freq(cdl=cdl)\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        pilot_matrix = get_pilot_matrix(response_symbols)\n",
    "        # pilot_matrix = replicate_to_shape(pilot_matrix, h_freq_shape)\n",
    "        pilot_matrix = reshape_data(pilot_matrix)\n",
    "        pilot_matrix = scaler.fit_transform(pilot_matrix, False).to(device)\n",
    "        h_est_hat, *values = vae(pilot_matrix)\n",
    "        h_est_hat = scaler.inverse_transform(h_est_hat.cpu().detach())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        ssim_score = ssim_func(\n",
    "            torch.from_numpy(h_est),\n",
    "            torch.from_numpy(h_est_hat),\n",
    "        )\n",
    "        ssim_scores.append(ssim_score)\n",
    "\n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "\n",
    "def ssim_channel_estimation_ls(\n",
    "    number_of_trials, inter: str = \"nn\", cdl=cdl, no=no, order=\"t-f\"\n",
    "):\n",
    "    ssim_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        h_freq = sampling_channel_freq(cdl=cdl)\n",
    "\n",
    "        # response signal on frequency domain via CDL channel model + AWGN noise\n",
    "        response_symbols = response_freqency_domain(mapped_qam_symbol, h_freq, no=no)\n",
    "\n",
    "        channel_estimator = ChannelEstimator(interpolation_factor=inter, order=order)\n",
    "        h_est_hat, err_var = channel_estimator.estimate(response_symbols)\n",
    "\n",
    "        h_est_hat = reshape_data(h_est_hat.numpy())\n",
    "\n",
    "        h_est = remove_nulls_subcarriers(h_freq)\n",
    "        h_est = reshape_data(h_est.numpy())\n",
    "\n",
    "        ssim_score = ssim_func(\n",
    "            torch.from_numpy(h_est_hat),\n",
    "            torch.from_numpy(h_est),\n",
    "        )\n",
    "        ssim_scores.append(ssim_score)\n",
    "\n",
    "    return np.mean(ssim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for AI: 0.048670053482055664 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ai(1)\n",
    "end = time.time()\n",
    "print(f\"Time taken for AI: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LS+NN: 0.05892300605773926 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ls(1, inter=\"nn\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+NN: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LS+Lin: 0.05888009071350098 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ls(1, inter=\"lin\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+Lin: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LS+LMMSE: 0.3992776870727539 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ssim_score = ssim_channel_estimation_ls(1, inter=\"lmmse\")\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+LMMSE: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 21.01it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.03it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.00it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.06it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.83it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.97it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.93it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.92it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.00it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.95it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.07it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.97it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.84it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.89it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.03it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.88it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.88it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.68it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.86it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.69it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.97it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.87it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.90it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.76it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.00it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.74it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "hyperparameters = {\n",
    "    \"ebno_dbs\": np.arange(-20, 21, 5),\n",
    "    \"speeds\": np.arange(0, 11, 5),\n",
    "}\n",
    "\n",
    "number_of_trials = 200\n",
    "ssim_scores = {}\n",
    "inference_times = {}  # Dictionary to store inference times\n",
    "\n",
    "for speed in hyperparameters[\"speeds\"]:\n",
    "    cdl = CDL(\n",
    "        cdl_model,\n",
    "        delay_spread,\n",
    "        carrier_frequency,\n",
    "        ut_array,\n",
    "        bs_array,\n",
    "        direction,\n",
    "        min_speed=speed,\n",
    "    )\n",
    "    for ebno_db in hyperparameters[\"ebno_dbs\"]:\n",
    "        ssim_score = {}\n",
    "        inference_time = {}  # Store times for each model\n",
    "        no = ebnodb2no(ebno_db, bits_per_symbol, code_rate, rg)\n",
    "\n",
    "        # Measure inference time for VAE\n",
    "        start_time = time.time()\n",
    "        ssim_score[\"vae\"] = ssim_channel_estimation_ai(number_of_trials, no=no, cdl=cdl)\n",
    "        inference_time[\"vae\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+NN\n",
    "        start_time = time.time()\n",
    "        ssim_score[\"ls+nn\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"nn\", cdl=cdl, no=no\n",
    "        )\n",
    "        inference_time[\"ls+nn\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LIN\n",
    "        start_time = time.time()\n",
    "        ssim_score[\"ls+lin\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lin\", cdl=cdl, no=no\n",
    "        )\n",
    "        inference_time[\"ls+lin\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        ssim_score[\"ls+lmmse: t-f\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"t-f\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: t-f\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        ssim_score[\"ls+lmmse: t-f-s\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"t-f-s\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: t-f-s\"] = (\n",
    "            time.time() - start_time\n",
    "        ) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        ssim_score[\"ls+lmmse: s-t-f\"] = ssim_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"s-t-f\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: s-t-f\"] = (\n",
    "            time.time() - start_time\n",
    "        ) / number_of_trials\n",
    "\n",
    "        ssim_scores[f\"speed_{speed}_ebno_db_{ebno_db}\"] = ssim_score\n",
    "        inference_times[f\"speed_{speed}_ebno_db_{ebno_db}\"] = inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval/ssim/ssim.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = ssim_scores\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval/ssim/time.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = inference_times\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ber_channel_estimation_ai(number_of_trials, no=no, cdl=cdl):\n",
    "    ber_scores = []\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        modulated_qam_symbols = ofdm_modulation(mapped_qam_symbol)\n",
    "        h_time = sampling_channel_time(cdl=cdl)\n",
    "        response_symbols = response_time_domain(modulated_qam_symbols, h_time, no=no)\n",
    "        demodulated_response_symbols = ofdm_demodulation(response_symbols)\n",
    "\n",
    "        # Channel estimation\n",
    "        pilot_matrix = get_pilot_matrix(demodulated_response_symbols)\n",
    "        # pilot_matrix = replicate_to_shape(pilot_matrix, h_freq_shape)\n",
    "        pilot_matrix = reshape_data(pilot_matrix)\n",
    "        pilot_matrix = scaler.fit_transform(pilot_matrix, False).to(device)\n",
    "        h_est_hat, *values = vae(pilot_matrix)\n",
    "        h_est_hat = scaler.inverse_transform(h_est_hat.cpu().detach())\n",
    "        h_est_hat = reversed_reshape_data(h_est_hat)\n",
    "\n",
    "        mapped_qam_symbol_hat, no_eff = lmmse_equalizer(\n",
    "            demodulated_response_symbols, h_est_hat, 0\n",
    "        )\n",
    "\n",
    "        binary_values_hat = qam_demapper(mapped_qam_symbol_hat, bits_per_symbol, no_eff)\n",
    "\n",
    "        decoded_binary_values_hat = ldpc_decoder(binary_values_hat)\n",
    "\n",
    "        error_rate = ber(binary_values, decoded_binary_values_hat)\n",
    "        ber_scores.append(error_rate)\n",
    "\n",
    "    return np.mean(ber_scores)\n",
    "\n",
    "\n",
    "def ber_channel_estimation_ls(\n",
    "    number_of_trials, inter: str = \"nn\", cdl=cdl, no=no, order=\"t-f\"\n",
    "):\n",
    "    for _ in tqdm(range(number_of_trials)):\n",
    "        binary_values = binary_sources(\n",
    "            [batch_size, num_tx, num_streams_per_tx, number_of_bits]\n",
    "        )\n",
    "        encoded_binary_values = ldpc_encoder(binary_values)\n",
    "        qam_symbols = qam_mapper(encoded_binary_values)\n",
    "        mapped_qam_symbol = resource_grid_mapper(qam_symbols)\n",
    "        modulated_qam_symbols = ofdm_modulation(mapped_qam_symbol)\n",
    "        h_time = sampling_channel_time(cdl=cdl)\n",
    "        response_symbols = response_time_domain(modulated_qam_symbols, h_time, no=no)\n",
    "        demodulated_response_symbols = ofdm_demodulation(response_symbols)\n",
    "\n",
    "        channel_estimator = ChannelEstimator(interpolation_factor=inter, order=order)\n",
    "        h_est_hat, err_var = channel_estimator.estimate(demodulated_response_symbols)\n",
    "        mapped_qam_symbol_hat, no_eff = lmmse_equalizer(\n",
    "            demodulated_response_symbols, h_est_hat, err_var\n",
    "        )\n",
    "\n",
    "        binary_values_hat = qam_demapper(mapped_qam_symbol_hat, bits_per_symbol, no_eff)\n",
    "\n",
    "        decoded_binary_values_hat = ldpc_decoder(binary_values_hat)\n",
    "\n",
    "        error_rate = ber(binary_values, decoded_binary_values_hat)\n",
    "\n",
    "    return float(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER AI:  0.0\n",
      "Time taken for AI: 0.2879765033721924 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ber_score = ber_channel_estimation_ai(1)\n",
    "print(\"BER AI: \", ber_score)\n",
    "end = time.time()\n",
    "print(f\"Time taken for AI: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER LS+NN:  0.0\n",
      "Time taken for LS+NN: 0.2737462520599365 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ber_score = ber_channel_estimation_ls(1, inter=\"nn\")\n",
    "print(\"BER LS+NN: \", ber_score)\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+NN: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER LS+LMMSE:  0.0\n",
      "Time taken for LS+LMMSE: 0.6273691654205322 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ber_score = ber_channel_estimation_ls(1, inter=\"lmmse\")\n",
    "print(\"BER LS+LMMSE: \", ber_score)\n",
    "end = time.time()\n",
    "print(f\"Time taken for LS+LMMSE: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:51<00:00,  3.89it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.88it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.89it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.88it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.88it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.86it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.86it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.89it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.89it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.88it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.85it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.88it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.86it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.86it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.86it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.89it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.88it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.88it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n",
      "100%|██████████| 200/200 [00:51<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "hyperparameters = {\n",
    "    \"ebno_dbs\": np.arange(-20, 21, 5),\n",
    "    \"speeds\": np.arange(0, 11, 5),\n",
    "}\n",
    "\n",
    "number_of_trials = 200\n",
    "ber_scores = {}\n",
    "inference_times = {}  # Dictionary to store inference times\n",
    "\n",
    "for speed in hyperparameters[\"speeds\"]:\n",
    "    cdl = CDL(\n",
    "        cdl_model,\n",
    "        delay_spread,\n",
    "        carrier_frequency,\n",
    "        ut_array,\n",
    "        bs_array,\n",
    "        direction,\n",
    "        min_speed=speed,\n",
    "    )\n",
    "    for ebno_db in hyperparameters[\"ebno_dbs\"]:\n",
    "        ber_score = {}\n",
    "        inference_time = {}  # Store times for each model\n",
    "        no = ebnodb2no(ebno_db, bits_per_symbol, code_rate, rg)\n",
    "\n",
    "        # Measure inference time for VAE\n",
    "        start_time = time.time()\n",
    "        ber_score[\"vae\"] = ber_channel_estimation_ai(number_of_trials, no=no, cdl=cdl)\n",
    "        inference_time[\"vae\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+NN\n",
    "        start_time = time.time()\n",
    "        ber_score[\"ls+nn\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"nn\", cdl=cdl, no=no\n",
    "        )\n",
    "        inference_time[\"ls+nn\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LIN\n",
    "        start_time = time.time()\n",
    "        ber_score[\"ls+lin\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lin\", cdl=cdl, no=no\n",
    "        )\n",
    "        inference_time[\"ls+lin\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        ber_score[\"ls+lmmse: t-f\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"t-f\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: t-f\"] = (time.time() - start_time) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        ber_score[\"ls+lmmse: t-f-s\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"t-f-s\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: t-f-s\"] = (\n",
    "            time.time() - start_time\n",
    "        ) / number_of_trials\n",
    "\n",
    "        # Measure inference time for LS+LMMSE\n",
    "        start_time = time.time()\n",
    "        ber_score[\"ls+lmmse: s-t-f\"] = ber_channel_estimation_ls(\n",
    "            number_of_trials, inter=\"lmmse\", cdl=cdl, no=no, order=\"s-t-f\"\n",
    "        )\n",
    "        inference_time[\"ls+lmmse: s-t-f\"] = (\n",
    "            time.time() - start_time\n",
    "        ) / number_of_trials\n",
    "\n",
    "        ber_scores[f\"speed_{speed}_ebno_db_{ebno_db}\"] = ber_score\n",
    "        inference_times[f\"speed_{speed}_ebno_db_{ebno_db}\"] = inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval/ber/ber.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = ber_scores\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = f\"{root_dir}/results/eval/ber/time.json\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "# Key for the current data\n",
    "key = f\"txant_{num_ut_ant}_rxant_{num_bs_ant}\"\n",
    "\n",
    "# Read existing data if the file exists\n",
    "existing_data = {}\n",
    "if os.path.exists(save_dir):\n",
    "    try:\n",
    "        with open(save_dir, \"r\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle empty or corrupted JSON file\n",
    "        existing_data = {}\n",
    "\n",
    "# Update or append the new data\n",
    "existing_data[key] = inference_times\n",
    "\n",
    "# Write back to the file\n",
    "with open(save_dir, \"w\") as f:\n",
    "    json.dump(existing_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ele6709",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
